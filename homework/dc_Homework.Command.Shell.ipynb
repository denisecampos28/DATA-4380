{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de462db",
   "metadata": {},
   "source": [
    "# Unix Shell\n",
    "\n",
    "There is a lot that can be done on the Unix shell command prompt. For homework, we will do some useful manipulations of CSV files.\n",
    "\n",
    "There is plenty of material online that will help you figure out how to do various tasks on the command line. Some example resources I found by googling:\n",
    "\n",
    "* Paths and Wildcards: https://www.warp.dev/terminus/linux-wildcards\n",
    "* General introduction to shell: https://github-pages.ucl.ac.uk/RCPSTrainingMaterials/HPCandHTCusingLegion/2_intro_to_shell.html\n",
    "* Manual pages: https://www.geeksforgeeks.org/linux-man-page-entries-different-types/?ref=ml_lbp\n",
    "* Chaining commands: https://www.geeksforgeeks.org/chaining-commands-in-linux/?ref=ml_lbp\n",
    "* Piping: https://www.geeksforgeeks.org/piping-in-unix-or-linux/\n",
    "* Using sed: https://www.geeksforgeeks.org/sed-command-linux-set-2/?ref=ml_lbp\n",
    "* Various Unix commands: https://www.geeksforgeeks.org/linux-commands/?ref=lbp\n",
    "* Cheat sheets:\n",
    "    * https://www.stationx.net/unix-commands-cheat-sheet/\n",
    "    * https://cheatography.com/davechild/cheat-sheets/linux-command-line/\n",
    "    * https://www.theknowledgeacademy.com/blog/unix-commands-cheat-sheet/\n",
    "    \n",
    "These aren't necessarily the best resources. Feel free to search for better ones. Also, don't forget that Unix has built-in manual pages for all of its commands. Just type `man <command>` at the command prompt. Use the space-bar to scroll through the documentation and \"q\" to exit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bc381",
   "metadata": {},
   "source": [
    "## Homework (Due Friday 6/13)\n",
    "\n",
    "### Setup \n",
    "\n",
    "1. Make sure you have setup:\n",
    "    * Laptop setup with python, jupyter, and usual Data Science stack install via pip.\n",
    "    * Note if you are using Windows, you must setup via WSL.\n",
    "        * Must know how to read files in Windows disks from WSL Ubuntu VM?\n",
    "        * Must know how to read files in your WSL Ubuntu VM from Windows?\n",
    "\n",
    "2. Make sure you are setup to use GitHub on the command line in you Linux / MacOS environment:\n",
    "    * Make sure GitHub is properly setup\n",
    "        * Authentication\n",
    "        * Demonstrate you can push from the command prompt.\n",
    "    * Create a new repository for you work. Name it appropriately. Make it public. \n",
    "    * Organize it. e.g. You will do various projects. e.g. Sub-directories for each project.\n",
    "\n",
    "3. Install Kaggle API:\n",
    "    * Install [kaggle API](https://www.kaggle.com/docs/api).\n",
    "        * Setup your PATH environment variable.\n",
    "        * Be able to edit text files\n",
    "\n",
    "4. Create a directory in your Linux / MacOS filesystem where you will store the files. We will be using CSV files from the Kaggle challenges and datasets listed here. Download and unzip all of the datasets **using the Kaggle API**.\n",
    "    * https://www.kaggle.com/competitions/diabetes-prediction-with-nn\n",
    "    * https://www.kaggle.com/datasets/rishidamarla/heart-disease-prediction\n",
    "    * https://www.kaggle.com/competitions/used-car-price-prediction-competition\n",
    "    * https://www.kaggle.com/datasets/yasserh/housing-prices-dataset\n",
    "    * https://www.kaggle.com/datasets/simtoor/mall-customers\n",
    "    * https://www.kaggle.com/competitions/business-research-methods-world-university-rankings\n",
    "    * https://www.kaggle.com/datasets/m5anas/cancer-patient-data-sets\n",
    "        \n",
    "5. Setup for homework submission.\n",
    "    * Create a homework directory in your GitHub Repo where you will submit your solutions. \n",
    "    * Create a symbolic link from this directory to where you stored the downloaded datasets in step 4 below.\n",
    "    * **Do not commit any datafiles into GitHub.**\n",
    "        \n",
    "### Exercises        \n",
    "\n",
    "Perform all of these tasks on the Unix command prompt. Some may require several commands. Many will require chaining commands together. Once you figure out how to perform the task, copy paste the command(s) into a notebook that you will submit.\n",
    "\n",
    "\n",
    "1. Organize your dataset directory. Make a new directory for the original zip files, and move the files there. In case you accidentally mess up one of the CSV files, you'll be able unzip the data again. \n",
    "\n",
    "Hint: use `mkdir` and `mv` commands with appropriate wildcards.\n",
    "\n",
    "2. The \"diabetes_prediction_dataset.csv\" file has a lot of entries. Create 3 new CSV files, each with about 1/3 of the data.\n",
    "\n",
    "Hints: \n",
    "* Use `head` to get first line.  \n",
    "* First create 3 files with just the first line by redirecting output of `head` into a file using `>`.\n",
    "* Use `wc` to count the number of entries\n",
    "* Chain/pipe `head` and `tail` to select specific lines, redirecting output to append to the 3 files you created using `>>`.\n",
    "\n",
    "\n",
    "3. Create 2 new CSV files from `Heart_Disease_Prediction.csv`, one containing rows with \"Presence\" label and another with \"Absence\" label. Make sure that the first line of each file contains the field names. \n",
    "\n",
    "Hints: \n",
    "* Use `head` to get first line.  \n",
    "* First create 2 files with just the first line by redirecting output of `head` into a file using `>`.\n",
    "* Use `grep` to select lines that contain \"Absence\" or \"Presence\" and append the output to the appropriate file created in the previous step.\n",
    "\n",
    "\n",
    "4. What fraction of cars in `car_web_scraped_dataset.csv` have had no accidents?\n",
    "\n",
    "Hints:\n",
    "* Use `grep` to select the appropriate lines.\n",
    "* Pipe the output of grep into `wc` (using `|`) to count the lines.\n",
    "\n",
    "\n",
    "5. Make the following replacements in `Housing.csv`, output the result into a new CSV:\n",
    "\n",
    "* yes -> 1\n",
    "* no -> 0\n",
    "* unfurnished -> 0\n",
    "* furnished -> 1\n",
    "* semi-furnished -> 2\n",
    "    \n",
    "Hints:\n",
    "* Use `sed` to do the replacement.\n",
    "* Use pipes to chain multiple `sed` commands.\n",
    "* To avoid replacing \"unfurnished\" or \"semi-furnished\" when performing the \"furnished\" replacement, try replacing \",furnished\" with \",1\".\n",
    "\n",
    "\n",
    "6. Create a new CSV files from `Mall_Customers`, removing \"CustomerID\" column.\n",
    "\n",
    "Hints:\n",
    "* Use `cut` command\n",
    "* Default separator for `cut` is the space character. For CSV, you have to use option `-d ','`.\n",
    "\n",
    "\n",
    "7. Create a new file that contains the sum of the following fields for each row:\n",
    "    * Research Quality Score\n",
    "    * Industry Score\n",
    "    * International Outlook\n",
    "    * Research Environment Score\n",
    "    \n",
    "Hints:\n",
    "* Use `cut` to select the correct columns.\n",
    "* Use `tr` to replace ',' with '+'.\n",
    "* Pipe output into `bc` to compute the sum.\n",
    "\n",
    "\n",
    "8. Sort the `cancer patient data sets.csv` file by age. Make sure the output is a readable CSV file.\n",
    "\n",
    "Hints:\n",
    "* Use sort with `-n`, `-t`, and `-k` options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657385f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.) \n",
    "(base) Denises-Air:kaggle_datasets denisecampos$ mkdir zips \n",
    "(base) Denises-Air:kaggle_datasets denisecampos$ mv *.zip zips/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fcae0-91ab-40c5-ae3f-692ad70fc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.)\n",
    "(base) Denises-Air:diabetes_prediction denisecampos$ head -n 1 diabetes_prediction_dataset.csv >\n",
    "diabetes_part1.csv \n",
    "(base) Denises-Air:diabetes_prediction denisecampos$ head -n 1 diabetes_prediction_dataset.csv >\n",
    "diabetes_part2.csv \n",
    "(base) Denises-Air:diabetes_prediction denisecampos$ head -n 1 diabetes_prediction_dataset.csv >\n",
    "diabetes_part3.csv \n",
    "tail -n +2 diabetes_prediction_dataset.csv | wc -l\n",
    "tail -n +2 diabetes_prediction_dataset.csv | head -n 33333 >> diabetes_part1.csv\n",
    "tail -n +33335 diabetes_prediction_dataset.csv | head -n 33333 >> diabetes_part2.csv\n",
    "tail -n +66668 diabetes_prediction_dataset.csv >> diabetes_part3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c4b66-65de-405d-a0bf-5b68e20a932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.)\n",
    "head -n 1 Heart_Disease_Prediction.csv > heart_presence.csv\n",
    "head -n 1 Heart_Disease_Prediction.csv > heart_absence.csv\n",
    "grep \"Presence\" Heart_Disease_Prediction.csv >> heart_presence.csv\n",
    "grep \"Absence\" Heart_Disease_Prediction.csv >> heart_absence.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb9cec-419e-45b8-ac0c-af2b164460fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.)\n",
    "grep -i \"no accident\" car_web_scraped_dataset.csv | wc -l\n",
    "tail -n +2 car_web_scraped_dataset.csv | wc -l\n",
    "\n",
    "2223/2840 = 78.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fbe2b0-f019-4f6d-b941-e96078e19f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.)\n",
    "cat Housing.csv \\\n",
    "| sed 's/,yes/,1/g' \\\n",
    "| sed 's/,no/,0/g' \\\n",
    "| sed 's/,unfurnished/,0/g' \\\n",
    "| sed 's/,semi-furnished/,2/g' \\\n",
    "| sed 's/,furnished/,1/g' \\\n",
    "> Housing_cleaned.csv]\n",
    "head Housing_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f5154-992a-4a80-8c52-1111caa856bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.)\n",
    "cut -d',' -f2- Mall_Customers.csv > Mall_Customers_noID.csv\n",
    "head Mall_Customers_noID.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfecea8-ba8e-4f30-a8fc-14873589c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.)\n",
    "tail -n +2 \"world all university rank and rank score.csv\" \\\n",
    "| cut -d',' -f5-8 \\\n",
    "| tr -d '\"' \\\n",
    "| tr ',' '+' \\\n",
    "| awk '{ if ($0 ~ /^[0-9.+]+$/) print | \"bc\" }' > university_score_sums.txt\n",
    "head university_score_sums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bf16d-849a-4afd-b28e-a77faf8f5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.)\n",
    "(head -n 1 \"cancer patient data sets.csv\" && tail -n +2 \"cancer patient data sets.csv\" | sort -t',' -k3 -n) > cancer_sorted_by_age.csv\n",
    "head cancer_sorted_by_age.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce55232-2eb3-473e-9436-0fa0cc681960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
